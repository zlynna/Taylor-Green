import tensorflow as tf
import numpy as np

class Net:
    def __init__(self, *inputs, layers):

        self.layers = layers
        self.num_layers = len(self.layers)
        # inputs = inputs.np()
        if len(inputs) == 0:
            in_dim = self.layers[0]
            self.X_mean = np.zeros([1, in_dim])
            self.X_std = np.ones([1, in_dim])
        else:
            X = np.concatenate(inputs, 1)
            self.X_mean = X.mean(0, keepdims=True)
            self.X_std = X.std(0, keepdims=True)
            self.X_min = X.min(0, keepdims=True)
            self.X_max = X.max(0, keepdims=True)

        self.weights = []
        self.biases = []
        self.gammas = []

        for l in range(0, self.num_layers - 1):
            in_dim = self.layers[l]
            out_dim = self.layers[l + 1]
            W = self.xavier_init(size=[in_dim, out_dim])
            b = self.xavier_init(size=[1, out_dim])
            g = self.xavier_init(size=[1, out_dim])
            # tensorflow variables
            self.weights.append(tf.Variable(W, dtype=tf.float32, trainable=True))
            self.biases.append(tf.Variable(b, dtype=tf.float32, trainable=True))
            self.gammas.append(tf.Variable(g, dtype=tf.float32, trainable=True))

    def xavier_init(self, size):
        in_dim = size[0]
        out_dim = size[1]
        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))
        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)

    def __call__(self, *inputs):

        H = 2 * (tf.concat(inputs, 1) - self.X_min) / (self.X_max - self.X_min) - 1  # data normalization, 方法不一样， 标准化与归一化
        # H = (tf.concat(inputs, 1) - self.X_mean) / self.X_std

        for l in range(0, self.num_layers - 1):
            W = self.weights[l]
            b = self.biases[l]
            g = self.gammas[l]
            # weight normalization
            V = W / tf.norm(W, axis=0, keepdims=True)  # 这里多了weight normalization
            # matrix multiplication
            H = tf.matmul(H, V)
            # add bias
            H = g * H + b
            # activation
            if l < self.num_layers - 2:
                H = tf.tanh(H)
        if H.shape[1] == 3:
            Y = tf.split(H, num_or_size_splits=H.shape[1], axis=1)
            return Y
        else:
            return H
